[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ゼロから作るDeep Learning in R",
    "section": "",
    "text": "ゼロから作るDeep Learning をRで実装しようとするものです。"
  },
  {
    "objectID": "ch02.html",
    "href": "ch02.html",
    "title": "2章 パーセプトロン",
    "section": "",
    "text": "ANDゲートの実装\n\nAND <- function(x1, x2) {\n  x <- c(x1, x2)\n  w <- c(0.5, 0.5)\n  b <- -0.7\n  tmp <- sum(w * x) + b\n\n  if (tmp <= 0) 0 else 1\n}\n\n\nAND(0, 0)\n\n[1] 0\n\nAND(0, 1)\n\n[1] 0\n\nAND(1, 0)\n\n[1] 0\n\nAND(1, 1)\n\n[1] 1\n\n\nゲートを作る補助関数\n\nmake_gate <- function(w, b) {\n  function(x1, x2) {\n    x <- c(x1, x2)\n    tmp <- sum(w * x) + b\n\n    if (tmp <= 0) 0 else 1\n  }\n}\n\nNANDゲートの実装\n\nNAND <- make_gate(w = c(-0.5, -0.5), b = 0.7)\n\n\nNAND(0, 0)\n\n[1] 1\n\nNAND(0, 1)\n\n[1] 1\n\nNAND(1, 0)\n\n[1] 1\n\nNAND(1, 1)\n\n[1] 0\n\n\nORゲートの実装\n\nOR <- make_gate(w = c(0.5, 0.5), b = -0.2)\n\n\nOR(0, 0)\n\n[1] 0\n\nOR(0, 1)\n\n[1] 1\n\nOR(1, 0)\n\n[1] 1\n\nOR(1, 1)\n\n[1] 1"
  },
  {
    "objectID": "ch02.html#多層パーセプトロン",
    "href": "ch02.html#多層パーセプトロン",
    "title": "2章 パーセプトロン",
    "section": "多層パーセプトロン",
    "text": "多層パーセプトロン\nXOR ゲートの実装\n\nXOR <- function(x1, x2) {\n  s1 <- NAND(x1, x2)\n  s2 <- OR(x1, x2)\n  AND(s1, s2)\n}\n\n\nXOR(0, 0)\n\n[1] 0\n\nXOR(0, 1)\n\n[1] 1\n\nXOR(1, 0)\n\n[1] 1\n\nXOR(1, 1)\n\n[1] 0"
  },
  {
    "objectID": "ch03.html",
    "href": "ch03.html",
    "title": "3章 ニューラルネットワーク",
    "section": "",
    "text": "ステップ関数の実装\n\nstep_function <- function(x) as.numeric(x > 0)\n\n\nstep_function(c(-1.0, 1.0, 2.0))\n\n[1] 0 1 1\n\n\n\ncurve(step_function, -5, 5)\n\n\n\n\nシグモイド関数の実装\n\nsigmoid <- function(x) 1 / (1 + exp(-x))\n\n\nsigmoid(c(-1.0, 1.0, 2.0))\n\n[1] 0.2689414 0.7310586 0.8807971\n\n\n\ncurve(sigmoid, -5, 5)\n\n\n\n\nReLU関数\n\nrelu <- function(x) ifelse(x > 0, x, 0)\n\n\nrelu(c(-1.0, 1.0, 2.0))\n\n[1] 0 1 2\n\n\n\ncurve(relu, -5, 5)"
  },
  {
    "objectID": "ch03.html#出力層の設計",
    "href": "ch03.html#出力層の設計",
    "title": "3章 ニューラルネットワーク",
    "section": "出力層の設計",
    "text": "出力層の設計\nソフトマックス関数の実装\n\nsoftmax <- function(a) {\n  exp_a <- exp(a)\n  exp_a / sum(exp_a)\n}\n\n\nsoftmax(c(0.3, 2.9, 4.0))\n\n[1] 0.01821127 0.24519181 0.73659691\n\n\nオーバーフローに関する問題\n\nsoftmax(c(1010, 1000, 990))\n\n[1] NaN NaN NaN\n\n\n対策\n\nsoftmax <- function(a) {\n  exp_a <- exp(a - max(a))\n  exp_a / sum(exp_a)\n}\n\n\nsoftmax(c(1010, 1000, 990))\n\n[1] 9.999546e-01 4.539787e-05 2.061060e-09"
  },
  {
    "objectID": "ch03.html#手書き数字認識",
    "href": "ch03.html#手書き数字認識",
    "title": "3章 ニューラルネットワーク",
    "section": "手書き数字認識",
    "text": "手書き数字認識\nMNISTデータセットのダウンロード、読み込みにdslabsパッケージを使う。\nディレクトリmnist_dirにデータをダウンロードすることにする。\n\nmnist_dir <- \"input/mnist/\"\nfile.exists(mnist_dir)\n\n[1] TRUE\n\n\nデータをダウンロードして読み込む。\n\nmnist <- dslabs::read_mnist(download = TRUE, destdir = mnist_dir)\n\n一度ダウンロードすれば、以降はダウンロードしたディレクトリからデータを読み込める。\n\nmnist <- dslabs::read_mnist(path = mnist_dir)\n\nデータ構造を確認\n\nstr(mnist)\n\nList of 2\n $ train:List of 2\n  ..$ images: int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ labels: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ...\n $ test :List of 2\n  ..$ images: int [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ labels: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ...\n\n\nMNIST画像を表示してみる。\n\nimg_show <- function(img) {\n  image(matrix(img, nrow = 28)[, 28:1], col = gray(1:12 / 12))\n}\n\nラベルを確認\n\nmnist$train$labels[1]\n\n[1] 5\n\n\n画像を表示\n\nimg_show(mnist$train$images[1, ])\n\n\n\n\n学習済みのパラメータ、sample_weight.pklをダウンロード\n\nurl <- \"https://github.com/oreilly-japan/deep-learning-from-scratch/raw/master/ch03/sample_weight.pkl\"\ndownload.file(url, \"input/sample_weight.pkl\")\n\nreticulateパッケージのpy_load_object関数を使ってpickleファイルを読み込む。\n\nnetwork <- reticulate::py_load_object(\"input/sample_weight.pkl\")\n\n中身を確認\n\nstr(network)\n\nList of 6\n $ b2: num [1:100(1d)] -0.01471 -0.07215 -0.00156 0.122 0.11603 ...\n $ W1: num [1:784, 1:50] -0.00741 -0.0103 -0.01309 -0.01001 0.02207 ...\n $ b1: num [1:50(1d)] -0.0675 0.0696 -0.0273 0.0226 -0.22 ...\n $ W2: num [1:50, 1:100] -0.1069 0.2991 0.0658 0.0939 0.048 ...\n $ W3: num [1:100, 1:10] -0.422 -0.524 0.683 0.155 0.505 ...\n $ b3: num [1:10(1d)] -0.06024 0.00933 -0.0136 0.02167 0.01074 ...\n\n\n推論処理を行うニューラルネットワークの実装\n\nrep_row <- function(x, n) matrix(rep(x, n), n, byrow = TRUE)\n\npredict <- function(network, x) {\n  n <- nrow(x)\n  if (is.null(n)) n <- 1\n\n  a1 <- x %*% network$W1 + rep_row(network$b1, n)\n  z1 <- sigmoid(a1)\n  a2 <- z1 %*% network$W2 + rep_row(network$b2, n)\n  z2 <- sigmoid(a2)\n  a3 <- z2 %*% network$W3 + rep_row(network$b3, n)\n\n  softmax(a3)\n}\n\n推論を実行\n\nlibrary(tidyverse)\n\n\n# 正規化\nimages <- mnist$test$images / 255\n\npreds <- 1:nrow(images) %>%\n  map(~ predict(network, images[., ])) %>%\n  map_int(which.max) - 1\n\n\nhead(preds)\n\n[1] 7 2 1 0 4 1\n\n\n認識精度\n\naccuracy <- mean(preds == mnist$test$labels)\naccuracy\n\n[1] 0.9352\n\n\n誤認識した画像を確認\n\nmisrecognitions <- tibble(actual = mnist$test$labels, pred = preds) %>%\n  mutate(i = row_number(), .before = 1) %>%\n  filter(actual != pred)\n\nmisrecognitions %>% head(12)\n\n# A tibble: 12 × 3\n       i actual  pred\n   <int>  <int> <dbl>\n 1     9      5     6\n 2    34      4     6\n 3    67      6     2\n 4    93      9     4\n 5   125      7     4\n 6   150      2     9\n 7   218      6     5\n 8   234      8     7\n 9   242      9     8\n10   246      3     5\n11   248      4     2\n12   260      6     0\n\n\n\npar(mfrow = c(3, 4))\n\nmisrecognitions %>%\n  head(12) %>%\n  pull(i) %>%\n  walk(~ img_show(mnist$test$images[., ]))\n\n\n\n\nバッチ処理による実行\n\nbatch_size <- 100\n\npreds2 <- seq(1, nrow(images), batch_size) %>%\n  map(~ predict(network, images[.:(. + batch_size - 1), ])) %>%\n  reduce(rbind) %>%\n  apply(1, which.max) - 1\n\nidentical(preds, preds2)\n\n[1] TRUE"
  },
  {
    "objectID": "ch04.html",
    "href": "ch04.html",
    "title": "4章 ニューラルネットワークの学習",
    "section": "",
    "text": "準備"
  },
  {
    "objectID": "ch04.html#損失関数",
    "href": "ch04.html#損失関数",
    "title": "4章 ニューラルネットワークの学習",
    "section": "損失関数",
    "text": "損失関数\n交差エントロピー誤差\n\ncross_entropy_error <- function(y, t) {\n  if (is.vector(y)) {\n    y <- matrix(y, 1)\n    t <- matrix(t, 1)\n  }\n\n  if (identical(dim(t), dim(y))) {\n    t <- apply(t, 1, which.max) - 1\n  }\n\n  batch_size <- nrow(y)\n\n  -sum(log(map2_dbl(1:batch_size, t, ~ y[.x, .y + 1]) + 1e-7)) / batch_size\n}\n\n\ny  <- c(0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0)\n\none-hot表現\n\nt <- c(0, 0, 1, 0, 0, 0, 0, 0, 0, 0)\ncross_entropy_error(y, t)\n\n[1] 0.5108255\n\n\nラベル\n\ncross_entropy_error(y, 2)\n\n[1] 0.5108255\n\n\nバッチデータ\n\ny2 <- matrix(seq(0.1, 0.6, 0.1), 2, byrow = TRUE)\nt2 <- matrix(c(0, 1, 0, 0, 0, 1), 2, byrow = TRUE)\ncross_entropy_error(y2, t2)\n\n[1] 1.060131\n\ncross_entropy_error(y2, c(1, 2))\n\n[1] 1.060131"
  },
  {
    "objectID": "ch04.html#勾配",
    "href": "ch04.html#勾配",
    "title": "4章 ニューラルネットワークの学習",
    "section": "勾配",
    "text": "勾配\n勾配\n\nnumerical_gradient <- function(f, x) {\n  h <- 1e-4\n  if (is.vector(x)) x <- matrix(x, 1)\n  grad <- matrix(0, nrow(x), ncol(x))\n\n  for (i in 1:nrow(x)) {\n    for (j in 1:ncol(x)) {\n      x1 <- identity(x)\n      x1[i, j] <- x[i, j] + h\n      x2 <- identity(x)\n      x2[i, j] <- x[i, j] - h\n      grad[i, j] <- (f(x1) - f(x2)) / (2 * h)\n    }\n  }\n\n  grad\n}\n\n\nfunction_2 <- function(x) sum(x ^ 2)\n\n\nnumerical_gradient(function_2, c(3.0, 4.0))\n\n     [,1] [,2]\n[1,]    6    8\n\nnumerical_gradient(function_2, c(0.0, 2.0))\n\n     [,1] [,2]\n[1,]    0    4\n\nnumerical_gradient(function_2, c(3.0, 0.0))\n\n     [,1] [,2]\n[1,]    6    0\n\n\n\n勾配法\n勾配降下法\n\ngradient_descent <- function(f, init_x, lr = 0.01, step_num = 100) {\n  x <- init_x\n  for (i in 1:step_num) {\n    x <- x - lr * numerical_gradient(f, x)\n  }\n  x\n}\n\nもしくは\n\ngradient_descent <- function(f, init_x, lr = 0.01, step_num = 100) {\n  reduce(1:step_num, ~ .x - lr * numerical_gradient(f, .x), .init = init_x)\n}\n\n\ninit_x <- c(-3.0, 4.0)\ngradient_descent(function_2, init_x, lr = 0.1, step_num = 100)\n\n              [,1]         [,2]\n[1,] -6.111108e-10 8.148144e-10\n\n\n勾配法による更新のプロセス\n\ngradient_descent_history <- function(f, init_x, lr = 0.01, step_num = 100) {\n  accumulate(1:step_num, ~ .x - lr * numerical_gradient(f, .x), .init = init_x)\n}\n\n\nx_history <- gradient_descent_history(function_2, matrix(init_x, 1), lr = 0.1, step_num = 20)\nx_history_df <- x_history %>%\n  reduce(rbind) %>%\n  as.data.frame %>%\n  set_names(\"x0\", \"x1\")\n\nhead(x_history_df)\n\n        x0      x1\n1 -3.00000 4.00000\n2 -2.40000 3.20000\n3 -1.92000 2.56000\n4 -1.53600 2.04800\n5 -1.22880 1.63840\n6 -0.98304 1.31072\n\n\n\nplot(x_history_df)\n\n\n\n\n\n\nニューラルネットワークに対する勾配\n\nSimpleNet <- R6Class(\"SimpleNet\", list(\n  W = NULL,\n  initialize = function(W) self$W <- W,\n  predict = function(x) x %*% self$W,\n  loss = function(x, t) {\n    z <- self$predict(x)\n    y <- softmax(z)\n    cross_entropy_error(y, t)\n  },\n  loss_function = function(x, t) {\n    w_orig <- self$W\n    function(w) {\n      self$W <- w\n      loss <- self$loss(x, t)\n      self$W <- w_orig\n      loss\n    }\n  }))\n\n\nW <- matrix(c(0.47355232, 0.9977393, 0.84668094,\n              0.85557411, 0.03563661, 0.69422093),\n            2, byrow = TRUE)\nnet <- SimpleNet$new(W)\nx <- c(0.6, 0.9)\np <- net$predict(x)\np\n\n         [,1]      [,2]     [,3]\n[1,] 1.054148 0.6307165 1.132807\n\nt <- matrix(c(0, 0, 1), 1)\nnet$loss(x, t)\n\n[1] 0.9280683\n\n\n勾配\n\nf <- net$loss_function(x, t)\nnumerical_gradient(f, net$W)\n\n          [,1]      [,2]      [,3]\n[1,] 0.2192476 0.1435624 -0.362810\n[2,] 0.3288714 0.2153436 -0.544215\n\n\nfは重みを受け取って損失を返す関数。"
  },
  {
    "objectID": "ch04.html#学習アルゴリズムの実装",
    "href": "ch04.html#学習アルゴリズムの実装",
    "title": "4章 ニューラルネットワークの学習",
    "section": "学習アルゴリズムの実装",
    "text": "学習アルゴリズムの実装\n\n2層ニューラルネットワークのクラス\n\nTwoLayerNet <- R6Class(\"TwoLayerNet\", list(\n  params = NULL,\n  initialize = function(input_size, hidden_size, output_size,\n                        weight_init_std = 0.01) {\n    self$params <- list()\n    self$params$W1 <- matrix(rnorm(input_size * hidden_size, sd = weight_init_std),\n                             input_size, hidden_size)\n    self$params$b1 <- matrix(0, ncol = hidden_size)\n    self$params$W2 <- matrix(rnorm(hidden_size * output_size, sd = weight_init_std),\n                             hidden_size, output_size)\n    self$params$b2 <- matrix(0, ncol = output_size)\n  },\n  predict = function(x) {\n    n <- nrow(x)\n    if (is.null(n)) n <- 1\n\n    a1 <- x %*% self$params$W1 + rep_row(self$params$b1, n)\n    z1 <- sigmoid(a1)\n    a2 <- z1 %*% self$params$W2 + rep_row(self$params$b2, n)\n\n    softmax(a2)\n  },\n  loss = function(x, t) {\n    y <- self$predict(x)\n    cross_entropy_error(y, t)\n  },\n  loss_function = function(name, x, t) {\n    w_orig <- self$params[[name]]\n    function(w) {\n      self$params[[name]] <- w\n      loss <- self$loss(x, t)\n      self$params[[name]] <- w_orig\n      loss\n    }\n  },\n  numerical_gradient = function(x, t) {\n    list(\n      W1 = numerical_gradient(self$loss_function(\"W1\", x, t), self$params$W1),\n      b1 = numerical_gradient(self$loss_function(\"b1\", x, t), self$params$b1),\n      W2 = numerical_gradient(self$loss_function(\"W2\", x, t), self$params$W2),\n      b2 = numerical_gradient(self$loss_function(\"b2\", x, t), self$params$b2)\n    )\n  }))\n\n\nnet <- TwoLayerNet$new(input_size = 784, hidden_size = 100, output_size = 10)\nnet$params %>% map(dim)\n\n$W1\n[1] 784 100\n\n$b1\n[1]   1 100\n\n$W2\n[1] 100  10\n\n$b2\n[1]  1 10\n\n\nダミーの入力データと正解ラベル\n\nx <- matrix(runif(100 * 784), 100)\nt <- matrix(runif(100 * 10), 100)\n\n勾配を計算\n\ngrads <- net$numerical_gradient(x, t)"
  },
  {
    "objectID": "ch05.html",
    "href": "ch05.html",
    "title": "5章 誤差逆伝播法",
    "section": "",
    "text": "準備"
  },
  {
    "objectID": "ch05.html#単純なレイヤの実装",
    "href": "ch05.html#単純なレイヤの実装",
    "title": "5章 誤差逆伝播法",
    "section": "単純なレイヤの実装",
    "text": "単純なレイヤの実装\n\n乗算レイヤの実装\n\nMulLayer <- R6Class(\"MulLayer\", list(\n  x = NULL,\n  y = NULL,\n  forward = function(x, y) {\n    self$x <- x\n    self$y <- y\n    x * y\n  },\n  backward = function(dout) {\n    dx <- dout * self$y\n    dy <- dout * self$x\n    c(dx, dy)\n  }))\n\n順伝播\n\napple <- 100\napple_num <- 2\ntax <- 1.1\n\nmul_apple_layer <- MulLayer$new()\nmul_tax_layer <- MulLayer$new()\n\napple_price <- mul_apple_layer$forward(apple, apple_num)\nprice <- mul_tax_layer$forward(apple_price, tax)\n\n\nprice\n\n[1] 220\n\n\n逆伝播（各変数に関する微分）\n\ndprice <- 1\nb1 <- mul_tax_layer$backward(dprice) %>%\n  set_names(c(\"dapple_price\", \"dtax\"))\nb2 <- mul_apple_layer$backward(b1[\"dapple_price\"]) %>%\n  set_names(c(\"dapple\", \"dapple_num\"))\n\n\nb2\n\n    dapple dapple_num \n       2.2      110.0 \n\nb1\n\ndapple_price         dtax \n         1.1        200.0 \n\n\n\n\n加算レイヤの実装\n\nAddLayer <- R6Class(\"AddLayer\", list(\n  forward = function(x, y) {\n    x + y\n  },\n  backward = function(dout) {\n    dx <- dout * 1\n    dy <- dout * 1\n    c(dx, dy)\n  }))\n\nリンゴ2個とみかん3個の買い物\n\napple <- 100\napple_num <- 2\norange <- 150\norange_num <- 3\ntax <- 1.1\n\n# layer\nmul_apple_layer <- MulLayer$new()\nmul_orange_layer <- MulLayer$new()\nadd_apple_orange_layer <- AddLayer$new()\nmul_tax_layer <- MulLayer$new()\n\n# forward\napple_price <- mul_apple_layer$forward(apple, apple_num)\norange_price <- mul_orange_layer$forward(orange, orange_num)\nall_price <- add_apple_orange_layer$forward(apple_price, orange_price)\nprice <- mul_tax_layer$forward(all_price, tax)\n\n# backward\ndprice <- 1\nb1 <- mul_tax_layer$backward(dprice) %>%\n  set_names(c(\"dall_price\", \"dtax\"))\nb2 <- add_apple_orange_layer$backward(b1[\"dall_price\"]) %>%\n  set_names(c(\"dapple_price\", \"dorange_price\"))\nb3 <- mul_orange_layer$backward(b2[\"dorange_price\"]) %>%\n  set_names(c(\"dorange\", \"dorange_num\"))\nb4 <- mul_apple_layer$backward(b2[\"dapple_price\"]) %>%\n  set_names(c(\"dapple\", \"dapple_num\"))\n\n\nprice\n\n[1] 715\n\nb4\n\n    dapple dapple_num \n       2.2      110.0 \n\nb3\n\n    dorange dorange_num \n        3.3       165.0 \n\nb2\n\n dapple_price dorange_price \n          1.1           1.1 \n\nb1\n\ndall_price       dtax \n       1.1      650.0"
  },
  {
    "objectID": "ch05.html#活性化関数レイヤの実装",
    "href": "ch05.html#活性化関数レイヤの実装",
    "title": "5章 誤差逆伝播法",
    "section": "活性化関数レイヤの実装",
    "text": "活性化関数レイヤの実装\n\nReLUレイヤ\n\nRelu <- R6Class(\"Relu\", list(\n  positive = NULL,\n  forward = function(x) {\n    self$positive = x > 0\n    x * self$positive\n  },\n  backward = function(dout) {\n    dout * self$positive\n  }))\n\n\n\nSigmoidレイヤ\n\nSigmoid <- R6Class(\"Sigmoid\", list(\n  out = NULL,\n  forward = function(x) {\n    self$out = 1 / (1 + exp(-x))\n    self$out\n  },\n  backward = function(dout) {\n    dout * (1 - self$out) * self$out\n  }))"
  },
  {
    "objectID": "ch05.html#affinesoftmaxレイヤの実装",
    "href": "ch05.html#affinesoftmaxレイヤの実装",
    "title": "5章 誤差逆伝播法",
    "section": "Affine/Softmaxレイヤの実装",
    "text": "Affine/Softmaxレイヤの実装\n\nバッチ版Affineレイヤ\n\nAffine <- R6Class(\"Affine\", list(\n  W = NULL,\n  b = NULL,\n  x = NULL,\n  dW = NULL,\n  db = NULL,\n  initialize = function(W, b) {\n    self$W <- W\n    self$b <- b\n  },\n  forward = function(x) {\n    self$x <- x\n    x %*% self$W + rep_row(self$b, nrow(x))\n  },\n  backward = function(dout) {\n    self$dW <- t(self$x) %*% dout\n    self$db <- matrix(apply(dout, 2, sum), 1)\n    dout %*% t(self$W)\n  }))\n\n\n\nSoftmax-with-Lossレイヤ\n\nSoftmaxWithLoss <- R6Class(\"SoftmaxWithLoss\", list(\n  loss = NULL,\n  y = NULL,\n  t = NULL,\n  forward = function(x, t) {\n    self$t <- t\n    self$y <- base::t(apply(x, 1, softmax))\n    self$loss <- cross_entropy_error(self$y, self$t)\n    self$loss\n  },\n  backward = function(dout) {\n    batch_size <- nrow(self$t)\n    (self$y - self$t) / batch_size\n  }))"
  },
  {
    "objectID": "ch05.html#誤差逆伝播法の実装",
    "href": "ch05.html#誤差逆伝播法の実装",
    "title": "5章 誤差逆伝播法",
    "section": "誤差逆伝播法の実装",
    "text": "誤差逆伝播法の実装\n\n誤差逆伝播法に対応したニューラルネットワークの実装\n\nTwoLayerNet <- R6Class(\"TwoLayerNet\", list(\n  params = NULL,\n  layers = NULL,\n  last_layer = NULL,\n  initialize = function(input_size, hidden_size, output_size,\n                        weight_init_std = 0.01) {\n    self$params <- list(\n      W1 = matrix(rnorm(input_size * hidden_size, sd = weight_init_std),\n                  input_size, hidden_size),\n      b1 = matrix(0, ncol = hidden_size),\n      W2 = matrix(rnorm(hidden_size * output_size, sd = weight_init_std),\n                  hidden_size, output_size),\n      b2 = matrix(0, ncol = output_size)\n    )\n\n    self$update_layers()\n  },\n  update_layers = function() {\n    self$layers <- list(\n      Affine1 = Affine$new(self$params$W1, self$params$b1),\n      Relu1 = Relu$new(),\n      Affine2 = Affine$new(self$params$W2, self$params$b2)\n    )\n\n    self$last_layer = SoftmaxWithLoss$new()\n  },\n  predict = function(x) {\n    for (layer in self$layers) {\n      x <- layer$forward(x)\n    }\n    x\n  },\n  loss = function(x, t) {\n    y <- self$predict(x)\n    self$last_layer$forward(y, t)\n  },\n  loss_function = function(name, x, t) {\n    w_orig <- self$params[[name]]\n    function(w) {\n      self$params[[name]] <- w\n      self$update_layers()\n      loss <- self$loss(x, t)\n      self$params[[name]] <- w_orig\n      self$update_layers()\n      loss\n    }\n  },\n  accuracy = function(x, t) {\n    y <- self$predict(x) %>%\n      apply(1, which.max) - 1\n\n    if (is.matrix(t)) {\n      t <- t %>%\n        apply(1, which.max) - 1\n    }\n\n    mean(y == t)\n  },\n  numerical_gradient = function(x, t) {\n    list(\n      W1 = numerical_gradient(self$loss_function(\"W1\", x, t), self$params$W1),\n      b1 = numerical_gradient(self$loss_function(\"b1\", x, t), self$params$b1),\n      W2 = numerical_gradient(self$loss_function(\"W2\", x, t), self$params$W2),\n      b2 = numerical_gradient(self$loss_function(\"b2\", x, t), self$params$b2)\n    )\n  },\n  gradient = function(x, t) {\n    # forward\n    self$loss(x, t)\n\n    # backward\n    dout <- 1\n    dout <- self$last_layer$backward(dout)\n\n    for (layer in rev(self$layers)) {\n      dout <- layer$backward(dout)\n    }\n\n    list(\n      W1 = self$layers$Affine1$dW,\n      b1 = self$layers$Affine1$db,\n      W2 = self$layers$Affine2$dW,\n      b2 = self$layers$Affine2$db\n    )\n  }))\n\n\n\n誤差逆伝播法の勾配確認\nラベルをone-hot表現に変換する関数\n\nto_one_hot <- function(labels) {\n  matrix(labels, 1) %>%\n    apply(2, function(x) {\n      v <- rep(0, 10)\n      v[x + 1] <- 1\n      v\n    }) %>%\n    t\n}\n\n\nto_one_hot(c(0, 2, 9))\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    0    0    0    0    0    0    0    0     0\n[2,]    0    0    1    0    0    0    0    0    0     0\n[3,]    0    0    0    0    0    0    0    0    0     1\n\n\n勾配確認\n\nmnist <- dslabs::read_mnist(path = \"input/mnist/\")\nx_train <- mnist$train$images / 255\nt_train <- to_one_hot(mnist$train$labels)\n\nset.seed(1)\nnetwork <- TwoLayerNet$new(input_size = 784, hidden_size = 50, output_size = 10)\n\nx_batch <- x_train %>% head(3)\nt_batch <- t_train %>% head(3)\n\ngrad_numerical <- network$numerical_gradient(x_batch, t_batch)\ngrad_backprop <- network$gradient(x_batch, t_batch)\n\nmap2(grad_numerical, grad_backprop, ~ mean(abs(.x - .y)))\n\n$W1\n[1] 4.062289e-10\n\n$b1\n[1] 2.386305e-09\n\n$W2\n[1] 4.953931e-09\n\n$b2\n[1] 1.403384e-07\n\n\n\n\n誤差逆伝播法を使った学習\n\nx_test <- mnist$test$images / 255\nt_test <- to_one_hot(mnist$test$labels)\n\nset.seed(1)\nnetwork <- TwoLayerNet$new(input_size = 784, hidden_size = 50, output_size = 10)\n\niters_num <- 10000\ntrain_size <- nrow(x_train)\nbatch_size <- 100\nlearning_rate <- 0.1\n\niter_per_epoch <- max(train_size %/% batch_size, 1)\n\nacc_df <- map_dfr(1:iters_num, function(i) {\n  batch_mask <- sample(train_size, batch_size)\n  x_batch <- x_train[batch_mask, ]\n  t_batch <- t_train[batch_mask, ]\n\n  grad <- network$gradient(x_batch, t_batch)\n\n  for (name in names(grad)) {\n    network$params[[name]] <- network$params[[name]] - learning_rate * grad[[name]]\n  }\n  network$update_layers()\n\n  loss <- network$loss(x_batch, t_batch)\n\n  train_acc <- test_acc <- NA\n  if (i %% iter_per_epoch == 1) {\n    train_acc <- network$accuracy(x_train, t_train)\n    test_acc <- network$accuracy(x_test, t_test)\n  }\n\n  lst(loss, train_acc, test_acc)\n})\n\n\nacc_df %>% filter(!is.na(train_acc))\n\n# A tibble: 17 × 3\n     loss train_acc test_acc\n    <dbl>     <dbl>    <dbl>\n 1 2.30      0.0872   0.0835\n 2 0.241     0.905    0.910 \n 3 0.254     0.923    0.926 \n 4 0.243     0.937    0.937 \n 5 0.0995    0.944    0.941 \n 6 0.114     0.950    0.949 \n 7 0.126     0.956    0.953 \n 8 0.0897    0.961    0.958 \n 9 0.0906    0.965    0.964 \n10 0.0401    0.965    0.961 \n11 0.150     0.967    0.964 \n12 0.0623    0.971    0.966 \n13 0.0763    0.974    0.969 \n14 0.0678    0.974    0.970 \n15 0.0427    0.975    0.970 \n16 0.0848    0.978    0.971 \n17 0.0259    0.978    0.971"
  },
  {
    "objectID": "ch06.html",
    "href": "ch06.html",
    "title": "6章 学習に関するテクニック",
    "section": "",
    "text": "準備"
  },
  {
    "objectID": "ch06.html#パラメータの更新",
    "href": "ch06.html#パラメータの更新",
    "title": "6章 学習に関するテクニック",
    "section": "パラメータの更新",
    "text": "パラメータの更新\n\nSGD\n\nSGD <- R6Class(\"SGD\", list(\n  lr = NULL,\n  initialize = function(lr = 0.01) {\n    self$lr <- lr\n  },\n  update = function(params, grads) {\n    for (name in names(grads)) {\n      params[[name]] <- params[[name]] - self$lr * grads[[name]]\n    }\n    params\n  }))\n\n\n\nMomentum\n\nMomentum <- R6Class(\"Momentum\", list(\n  lr = NULL,\n  momentum = NULL,\n  v = NULL,\n  initialize = function(lr = 0.01, momentum = 0.9) {\n    self$lr <- lr\n    self$momentum <- momentum\n  },\n  update = function(params, grads) {\n    if (is.null(self$v)) {\n      self$v <- map(params, ~ 0)\n    }\n    for (name in names(params)) {\n      self$v[[name]] <- self$momentum * self$v[[name]] - self$lr * grads[[name]]\n      params[[name]] <- params[[name]] + self$v[[name]]\n    }\n    params\n  }))\n\n\n\nAdaGrad\n\nAdaGrad <- R6Class(\"AdaGrad\", list(\n  lr = NULL,\n  h = NULL,\n  initialize = function(lr = 0.01) {\n    self$lr <- lr\n  },\n  update = function(params, grads) {\n    if (is.null(self$h)) {\n      self$h <- map(params, ~ 0)\n    }\n    for (name in names(params)) {\n      self$h[[name]] <- self$h[[name]] + grads[[name]] ^ 2\n      params[[name]] <- params[[name]] - self$lr * grads[[name]] / (sqrt(self$h[[name]]) + 1e-7)\n    }\n    params\n  }))\n\n\n\nAdam\n\nAdam <- R6Class(\"Adam\", list(\n  lr = NULL,\n  beta1 = NULL,\n  beta2 = NULL,\n  iter = NULL,\n  m = NULL,\n  v = NULL,\n  initialize = function(lr = 0.001, beta1 = 0.9, beta2 = 0.999) {\n    self$lr <- lr\n    self$beta1 <- beta1\n    self$beta2 <- beta2\n    self$iter <- 0\n  },\n  update = function(params, grads) {\n    if (is.null(self$m)) {\n      self$m <- map(params, ~ 0)\n      self$v <- map(params, ~ 0)\n    }\n    self$iter <- self$iter + 1\n    lr_t <- self$lr * sqrt(1 - self$beta2 ^ self$iter) / (1 - self$beta1 ^ self$iter)\n    for (name in names(params)) {\n      self$m[[name]] <- self$m[[name]] + (1 - self$beta1) * (grads[[name]] - self$m[[name]])\n      self$v[[name]] <- self$v[[name]] + (1 - self$beta2) * (grads[[name]]^2 - self$v[[name]])\n      params[[name]] <- params[[name]] - lr_t * self$m[[name]] / (sqrt(self$v[[name]]) + 1e-7)\n    }\n    params\n  }))\n\n\n\nどの更新手法を用いるか？\n\nf <- function(x, y) x^2 / 20 + y^2\ndf <- function(x, y) list(x / 10, 2 * y)\n\ninit_pos <- list(x = -7.0, y = 2.0)\nparams <- init_pos\n\noptimizers <- list(\n  SGD = SGD$new(lr = 0.95),\n  Momentum = Momentum$new(lr = 0.1),\n  AdaGrad = AdaGrad$new(lr = 1.5),\n  Adam = Adam$new(lr = 0.3)\n)\n\nupdate <- function(optimizer, init_pos, n = 30) {\n  params <- init_pos\n  map_dfr(1:n, function(i) {\n    grads <- df(params$x, params$y) %>%\n      set_names(c(\"x\", \"y\"))\n    params <<- optimizer$update(params, grads)\n  }) %>%\n    rbind(init_pos, .)\n}\n\nresults <- map(optimizers, update, init_pos)\n\nresults %>%\n  imap_dfr(~ mutate(.x, optimizer = .y)) %>%\n  mutate(optimizer = factor(optimizer, levels = names(optimizers))) %>%\n  ggplot(aes(x, y)) +\n  geom_line() +\n  facet_wrap(vars(optimizer), nrow = 2)"
  },
  {
    "objectID": "ch06.html#重みの初期値",
    "href": "ch06.html#重みの初期値",
    "title": "6章 学習に関するテクニック",
    "section": "重みの初期値",
    "text": "重みの初期値\n\n隠れ層のアクティベーション分布\n\nnode_num <- 100\nhidden_layer_size <- 5\nn <- 1000\n\nset.seed(1)\nx1 <- matrix(rnorm(n * node_num), n, node_num)\n\naccum_activations <- function(init, activation = sigmoid) {\n  accumulate(\n    1:hidden_layer_size,\n    function(x, i) {\n      w <- matrix(rnorm(node_num * node_num), node_num, node_num) * init\n      z <- x %*% w\n      activation(z)\n    },\n    .init = x1\n  ) %>% tail(-1)\n}\n\nplot_activations <- function(activations) {\n  activation_df <- activations %>%\n    imap_dfr(~ tibble(layer = .y, a = as.vector(.x)))\n\n  ggplot(activation_df, aes(x = a)) +\n    geom_histogram(binwidth = 0.02) +\n    coord_cartesian(xlim = c(0, 1)) +\n    facet_wrap(vars(layer), nrow = 1)\n}\n\n0と1に偏ったデータ分布\n\nactivations <- accum_activations(1)\nplot_activations(activations)\n\n\n\n\n重みの標準偏差を0.01にする。\n\nactivations <- accum_activations(0.01)\nplot_activations(activations)\n\n\n\n\nXavierの初期値を使う。\n\nactivations <- accum_activations(sqrt(1 / node_num))\nplot_activations(activations)\n\n\n\n\n\n\nReLUの場合の重みの初期値\n\nrelu <- function(x) ifelse(x > 0, x, 0)\n\n標準偏差が0.01のガウス分布を重みの初期値とした場合\n\nactivations <- accum_activations(0.01, relu)\nplot_activations(activations)\n\n\n\n\nXavierの初期値の場合\n\nactivations <- accum_activations(sqrt(1 / node_num), relu)\nplot_activations(activations)\n\n\n\n\nHeの初期値の場合\n\nactivations <- accum_activations(sqrt(2 / node_num), relu)\nplot_activations(activations)"
  },
  {
    "objectID": "ch06.html#batch-normalization",
    "href": "ch06.html#batch-normalization",
    "title": "6章 学習に関するテクニック",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nTODO"
  },
  {
    "objectID": "ch06.html#正則化",
    "href": "ch06.html#正則化",
    "title": "6章 学習に関するテクニック",
    "section": "正則化",
    "text": "正則化\nTODO"
  },
  {
    "objectID": "ch06.html#ハイパーパラメータの検証",
    "href": "ch06.html#ハイパーパラメータの検証",
    "title": "6章 学習に関するテクニック",
    "section": "ハイパーパラメータの検証",
    "text": "ハイパーパラメータの検証\nTODO"
  }
]