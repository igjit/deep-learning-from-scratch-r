[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ゼロから作るDeep Learning in R",
    "section": "",
    "text": "ゼロから作るDeep Learning をRで実装しようとするものです。"
  },
  {
    "objectID": "ch02.html",
    "href": "ch02.html",
    "title": "2章 パーセプトロン",
    "section": "",
    "text": "ANDゲートの実装\n\nAND <- function(x1, x2) {\n  x <- c(x1, x2)\n  w <- c(0.5, 0.5)\n  b <- -0.7\n  tmp <- sum(w * x) + b\n\n  if (tmp <= 0) 0 else 1\n}\n\n\nAND(0, 0)\n\n[1] 0\n\nAND(0, 1)\n\n[1] 0\n\nAND(1, 0)\n\n[1] 0\n\nAND(1, 1)\n\n[1] 1\n\n\nゲートを作る補助関数\n\nmake_gate <- function(w, b) {\n  function(x1, x2) {\n    x <- c(x1, x2)\n    tmp <- sum(w * x) + b\n\n    if (tmp <= 0) 0 else 1\n  }\n}\n\nNANDゲートの実装\n\nNAND <- make_gate(w = c(-0.5, -0.5), b = 0.7)\n\n\nNAND(0, 0)\n\n[1] 1\n\nNAND(0, 1)\n\n[1] 1\n\nNAND(1, 0)\n\n[1] 1\n\nNAND(1, 1)\n\n[1] 0\n\n\nORゲートの実装\n\nOR <- make_gate(w = c(0.5, 0.5), b = -0.2)\n\n\nOR(0, 0)\n\n[1] 0\n\nOR(0, 1)\n\n[1] 1\n\nOR(1, 0)\n\n[1] 1\n\nOR(1, 1)\n\n[1] 1"
  },
  {
    "objectID": "ch02.html#多層パーセプトロン",
    "href": "ch02.html#多層パーセプトロン",
    "title": "2章 パーセプトロン",
    "section": "多層パーセプトロン",
    "text": "多層パーセプトロン\nXOR ゲートの実装\n\nXOR <- function(x1, x2) {\n  s1 <- NAND(x1, x2)\n  s2 <- OR(x1, x2)\n  AND(s1, s2)\n}\n\n\nXOR(0, 0)\n\n[1] 0\n\nXOR(0, 1)\n\n[1] 1\n\nXOR(1, 0)\n\n[1] 1\n\nXOR(1, 1)\n\n[1] 0"
  },
  {
    "objectID": "ch03.html",
    "href": "ch03.html",
    "title": "3章 ニューラルネットワーク",
    "section": "",
    "text": "ステップ関数の実装\n\nstep_function <- function(x) as.numeric(x > 0)\n\n\nstep_function(c(-1.0, 1.0, 2.0))\n\n[1] 0 1 1\n\n\n\ncurve(step_function, -5, 5)\n\n\n\n\nシグモイド関数の実装\n\nsigmoid <- function(x) 1 / (1 + exp(-x))\n\n\nsigmoid(c(-1.0, 1.0, 2.0))\n\n[1] 0.2689414 0.7310586 0.8807971\n\n\n\ncurve(sigmoid, -5, 5)\n\n\n\n\nReLU関数\n\nrelu <- function(x) ifelse(x > 0, x, 0)\n\n\nrelu(c(-1.0, 1.0, 2.0))\n\n[1] 0 1 2\n\n\n\ncurve(relu, -5, 5)"
  },
  {
    "objectID": "ch03.html#出力層の設計",
    "href": "ch03.html#出力層の設計",
    "title": "3章 ニューラルネットワーク",
    "section": "出力層の設計",
    "text": "出力層の設計\nソフトマックス関数の実装\n\nsoftmax <- function(a) {\n  exp_a <- exp(a)\n  exp_a / sum(exp_a)\n}\n\n\nsoftmax(c(0.3, 2.9, 4.0))\n\n[1] 0.01821127 0.24519181 0.73659691\n\n\nオーバーフローに関する問題\n\nsoftmax(c(1010, 1000, 990))\n\n[1] NaN NaN NaN\n\n\n対策\n\nsoftmax <- function(a) {\n  exp_a <- exp(a - max(a))\n  exp_a / sum(exp_a)\n}\n\n\nsoftmax(c(1010, 1000, 990))\n\n[1] 9.999546e-01 4.539787e-05 2.061060e-09"
  },
  {
    "objectID": "ch03.html#手書き数字認識",
    "href": "ch03.html#手書き数字認識",
    "title": "3章 ニューラルネットワーク",
    "section": "手書き数字認識",
    "text": "手書き数字認識\nMNISTデータセットのダウンロード、読み込みにdslabsパッケージを使う。\nディレクトリmnist_dirにデータをダウンロードすることにする。\n\nmnist_dir <- \"input/mnist/\"\nfile.exists(mnist_dir)\n\n[1] TRUE\n\n\nデータをダウンロードして読み込む。\n\nmnist <- dslabs::read_mnist(download = TRUE, destdir = mnist_dir)\n\n一度ダウンロードすれば、以降はダウンロードしたディレクトリからデータを読み込める。\n\nmnist <- dslabs::read_mnist(path = mnist_dir)\n\nデータ構造を確認\n\nstr(mnist)\n\nList of 2\n $ train:List of 2\n  ..$ images: int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ labels: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ...\n $ test :List of 2\n  ..$ images: int [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ labels: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ...\n\n\nMNIST画像を表示してみる。\n\nimg_show <- function(img) {\n  image(matrix(img, nrow = 28)[, 28:1], col = gray(1:12 / 12))\n}\n\nラベルを確認\n\nmnist$train$labels[1]\n\n[1] 5\n\n\n画像を表示\n\nimg_show(mnist$train$images[1, ])\n\n\n\n\n学習済みのパラメータ、sample_weight.pklをダウンロード\n\nurl <- \"https://github.com/oreilly-japan/deep-learning-from-scratch/raw/master/ch03/sample_weight.pkl\"\ndownload.file(url, \"input/sample_weight.pkl\")\n\nreticulateパッケージのpy_load_object関数を使ってpickleファイルを読み込む。\n\nnetwork <- reticulate::py_load_object(\"input/sample_weight.pkl\")\n\n中身を確認\n\nstr(network)\n\nList of 6\n $ b2: num [1:100(1d)] -0.01471 -0.07215 -0.00156 0.122 0.11603 ...\n $ W1: num [1:784, 1:50] -0.00741 -0.0103 -0.01309 -0.01001 0.02207 ...\n $ b1: num [1:50(1d)] -0.0675 0.0696 -0.0273 0.0226 -0.22 ...\n $ W2: num [1:50, 1:100] -0.1069 0.2991 0.0658 0.0939 0.048 ...\n $ W3: num [1:100, 1:10] -0.422 -0.524 0.683 0.155 0.505 ...\n $ b3: num [1:10(1d)] -0.06024 0.00933 -0.0136 0.02167 0.01074 ...\n\n\n推論処理を行うニューラルネットワークの実装\n\nrep_row <- function(x, n) matrix(rep(x, n), n, byrow = TRUE)\n\npredict <- function(network, x) {\n  n <- nrow(x)\n  if (is.null(n)) n <- 1\n\n  a1 <- x %*% network$W1 + rep_row(network$b1, n)\n  z1 <- sigmoid(a1)\n  a2 <- z1 %*% network$W2 + rep_row(network$b2, n)\n  z2 <- sigmoid(a2)\n  a3 <- z2 %*% network$W3 + rep_row(network$b3, n)\n\n  softmax(a3)\n}\n\n推論を実行\n\nlibrary(tidyverse)\n\n\n# 正規化\nimages <- mnist$test$images / 255\n\npreds <- 1:nrow(images) %>%\n  map(~ predict(network, images[., ])) %>%\n  map_int(which.max) - 1\n\n\nhead(preds)\n\n[1] 7 2 1 0 4 1\n\n\n認識精度\n\naccuracy <- mean(preds == mnist$test$labels)\naccuracy\n\n[1] 0.9352\n\n\n誤認識した画像を確認\n\nmisrecognitions <- tibble(actual = mnist$test$labels, pred = preds) %>%\n  mutate(i = row_number(), .before = 1) %>%\n  filter(actual != pred)\n\nmisrecognitions %>% head(12)\n\n# A tibble: 12 × 3\n       i actual  pred\n   <int>  <int> <dbl>\n 1     9      5     6\n 2    34      4     6\n 3    67      6     2\n 4    93      9     4\n 5   125      7     4\n 6   150      2     9\n 7   218      6     5\n 8   234      8     7\n 9   242      9     8\n10   246      3     5\n11   248      4     2\n12   260      6     0\n\n\n\npar(mfrow = c(3, 4))\n\nmisrecognitions %>%\n  head(12) %>%\n  pull(i) %>%\n  walk(~ img_show(mnist$test$images[., ]))\n\n\n\n\nバッチ処理による実行\n\nbatch_size <- 100\n\npreds2 <- seq(1, nrow(images), batch_size) %>%\n  map(~ predict(network, images[.:(. + batch_size - 1), ])) %>%\n  reduce(rbind) %>%\n  apply(1, which.max) - 1\n\nidentical(preds, preds2)\n\n[1] TRUE"
  }
]