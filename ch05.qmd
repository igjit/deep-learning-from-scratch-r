# 5章 誤差逆伝播法 {.unnumbered}

準備

```{r}
#| output: false

library(tidyverse)
library(R6)
```

```{r}
source("common/functions.R")
```

## 単純なレイヤの実装

### 乗算レイヤの実装

```{r}
MulLayer <- R6Class("MulLayer", list(
  x = NULL,
  y = NULL,
  forward = function(x, y) {
    self$x <- x
    self$y <- y
    x * y
  },
  backward = function(dout) {
    dx <- dout * self$y
    dy <- dout * self$x
    c(dx, dy)
  }))
```

順伝播

```{r}
apple <- 100
apple_num <- 2
tax <- 1.1

mul_apple_layer <- MulLayer$new()
mul_tax_layer <- MulLayer$new()

apple_price <- mul_apple_layer$forward(apple, apple_num)
price <- mul_tax_layer$forward(apple_price, tax)
```

```{r}
price
```

逆伝播（各変数に関する微分）

```{r}
dprice <- 1
b1 <- mul_tax_layer$backward(dprice) %>%
  set_names(c("dapple_price", "dtax"))
b2 <- mul_apple_layer$backward(b1["dapple_price"]) %>%
  set_names(c("dapple", "dapple_num"))
```

```{r}
b2
b1
```

### 加算レイヤの実装

```{r}
AddLayer <- R6Class("AddLayer", list(
  forward = function(x, y) {
    x + y
  },
  backward = function(dout) {
    dx <- dout * 1
    dy <- dout * 1
    c(dx, dy)
  }))
```

リンゴ2個とみかん3個の買い物

```{r}
apple <- 100
apple_num <- 2
orange <- 150
orange_num <- 3
tax <- 1.1

# layer
mul_apple_layer <- MulLayer$new()
mul_orange_layer <- MulLayer$new()
add_apple_orange_layer <- AddLayer$new()
mul_tax_layer <- MulLayer$new()

# forward
apple_price <- mul_apple_layer$forward(apple, apple_num)
orange_price <- mul_orange_layer$forward(orange, orange_num)
all_price <- add_apple_orange_layer$forward(apple_price, orange_price)
price <- mul_tax_layer$forward(all_price, tax)

# backward
dprice <- 1
b1 <- mul_tax_layer$backward(dprice) %>%
  set_names(c("dall_price", "dtax"))
b2 <- add_apple_orange_layer$backward(b1["dall_price"]) %>%
  set_names(c("dapple_price", "dorange_price"))
b3 <- mul_orange_layer$backward(b2["dorange_price"]) %>%
  set_names(c("dorange", "dorange_num"))
b4 <- mul_apple_layer$backward(b2["dapple_price"]) %>%
  set_names(c("dapple", "dapple_num"))
```

```{r}
price
b4
b3
b2
b1
```

## 活性化関数レイヤの実装

### ReLUレイヤ

```{r}
Relu <- R6Class("Relu", list(
  positive = NULL,
  forward = function(x) {
    self$positive = x > 0
    x * self$positive
  },
  backward = function(dout) {
    dout * self$positive
  }))
```

### Sigmoidレイヤ

```{r}
Sigmoid <- R6Class("Sigmoid", list(
  out = NULL,
  forward = function(x) {
    self$out = 1 / (1 + exp(-x))
    self$out
  },
  backward = function(dout) {
    dout * (1 - self$out) * self$out
  }))
```

## Affine/Softmaxレイヤの実装

### バッチ版Affineレイヤ

```{r}
Affine <- R6Class("Affine", list(
  W = NULL,
  b = NULL,
  x = NULL,
  dW = NULL,
  db = NULL,
  initialize = function(W, b) {
    self$W <- W
    self$b <- b
  },
  forward = function(x) {
    self$x <- x
    x %*% self$W + rep_row(self$b, nrow(x))
  },
  backward = function(dout) {
    self$dW <- t(self$x) %*% dout
    self$db <- matrix(apply(dout, 2, sum), 1)
    dout %*% t(self$W)
  }))
```

### Softmax-with-Lossレイヤ

```{r}
SoftmaxWithLoss <- R6Class("SoftmaxWithLoss", list(
  loss = NULL,
  y = NULL,
  t = NULL,
  forward = function(x, t) {
    self$t <- t
    self$y <- base::t(apply(x, 1, softmax))
    self$loss <- cross_entropy_error(self$y, self$t)
    self$loss
  },
  backward = function(dout) {
    batch_size <- nrow(self$t)
    (self$y - self$t) / batch_size
  }))
```

## 誤差逆伝播法の実装

### 誤差逆伝播法に対応したニューラルネットワークの実装

```{r}
TwoLayerNet <- R6Class("TwoLayerNet", list(
  params = NULL,
  layers = NULL,
  last_layer = NULL,
  initialize = function(input_size, hidden_size, output_size,
                        weight_init_std = 0.01) {
    self$params <- list(
      W1 = matrix(rnorm(input_size * hidden_size, sd = weight_init_std),
                  input_size, hidden_size),
      b1 = matrix(0, ncol = hidden_size),
      W2 = matrix(rnorm(hidden_size * output_size, sd = weight_init_std),
                  hidden_size, output_size),
      b2 = matrix(0, ncol = output_size)
    )

    self$update_layers()
  },
  update_layers = function() {
    self$layers <- list(
      Affine1 = Affine$new(self$params$W1, self$params$b1),
      Relu1 = Relu$new(),
      Affine2 = Affine$new(self$params$W2, self$params$b2)
    )

    self$last_layer = SoftmaxWithLoss$new()
  },
  predict = function(x) {
    for (layer in self$layers) {
      x <- layer$forward(x)
    }
    x
  },
  loss = function(x, t) {
    y <- self$predict(x)
    self$last_layer$forward(y, t)
  },
  loss_function = function(name, x, t) {
    w_orig <- self$params[[name]]
    function(w) {
      self$params[[name]] <- w
      self$update_layers()
      loss <- self$loss(x, t)
      self$params[[name]] <- w_orig
      self$update_layers()
      loss
    }
  },
  accuracy = function(x, t) {
    y <- self$predict(x) %>%
      apply(1, which.max) - 1

    if (is.matrix(t)) {
      t <- t %>%
        apply(1, which.max) - 1
    }

    mean(y == t)
  },
  numerical_gradient = function(x, t) {
    list(
      W1 = numerical_gradient(self$loss_function("W1", x, t), self$params$W1),
      b1 = numerical_gradient(self$loss_function("b1", x, t), self$params$b1),
      W2 = numerical_gradient(self$loss_function("W2", x, t), self$params$W2),
      b2 = numerical_gradient(self$loss_function("b2", x, t), self$params$b2)
    )
  },
  gradient = function(x, t) {
    # forward
    self$loss(x, t)

    # backward
    dout <- 1
    dout <- self$last_layer$backward(dout)

    for (layer in rev(self$layers)) {
      dout <- layer$backward(dout)
    }

    list(
      W1 = self$layers$Affine1$dW,
      b1 = self$layers$Affine1$db,
      W2 = self$layers$Affine2$dW,
      b2 = self$layers$Affine2$db
    )
  }))
```

### 誤差逆伝播法の勾配確認

ラベルをone-hot表現に変換する関数

```{r}
to_one_hot <- function(labels) {
  matrix(labels, 1) %>%
    apply(2, function(x) {
      v <- rep(0, 10)
      v[x + 1] <- 1
      v
    }) %>%
    t
}
```

```{r}
to_one_hot(c(0, 2, 9))
```

勾配確認

```{r}
mnist <- dslabs::read_mnist(path = "input/mnist/")
x_train <- mnist$train$images / 255
t_train <- to_one_hot(mnist$train$labels)

set.seed(1)
network <- TwoLayerNet$new(input_size = 784, hidden_size = 50, output_size = 10)

x_batch <- x_train %>% head(3)
t_batch <- t_train %>% head(3)

grad_numerical <- network$numerical_gradient(x_batch, t_batch)
grad_backprop <- network$gradient(x_batch, t_batch)

map2(grad_numerical, grad_backprop, ~ mean(abs(.x - .y)))
```

### 誤差逆伝播法を使った学習

```{r}
x_test <- mnist$test$images / 255
t_test <- to_one_hot(mnist$test$labels)

set.seed(1)
network <- TwoLayerNet$new(input_size = 784, hidden_size = 50, output_size = 10)

iters_num <- 10000
train_size <- nrow(x_train)
batch_size <- 100
learning_rate <- 0.1

iter_per_epoch <- max(train_size %/% batch_size, 1)

acc_df <- map_dfr(1:iters_num, function(i) {
  batch_mask <- sample(train_size, batch_size)
  x_batch <- x_train[batch_mask, ]
  t_batch <- t_train[batch_mask, ]

  grad <- network$gradient(x_batch, t_batch)

  for (name in names(grad)) {
    network$params[[name]] <- network$params[[name]] - learning_rate * grad[[name]]
  }
  network$update_layers()

  loss <- network$loss(x_batch, t_batch)

  train_acc <- test_acc <- NA
  if (i %% iter_per_epoch == 1) {
    train_acc <- network$accuracy(x_train, t_train)
    test_acc <- network$accuracy(x_test, t_test)
  }

  lst(loss, train_acc, test_acc)
})
```

```{r}
acc_df %>% filter(!is.na(train_acc))
```
